# 0630
## 오늘 목표
```
1. Dataset 확인 및 활용방안 고민 (MIMIC-3)
Bio Signal에 대한 이해 : https://blog.lgcns.com/1815
Open source EMR Records : https://mimic.physionet.org/about/mimic/

2. Mathematics
Markov Chain
MCMC
BM
RBM

3. 기존 프로젝트 복기
Stacked Autoencoders
RBM
DNN + SVD / Gaussian Flattening
```


# 2. Mathematics

## Markov Chain
https://ko.wikipedia.org/wiki/%EB%A7%88%EB%A5%B4%EC%BD%94%ED%94%84_%EC%97%B0%EC%87%84<br>
https://brilliant.org/wiki/markov-chains/<br>
Markov Chain은 이산시간 확률 과정으로, 시간에 따른 계(System)의 상태 변화를 나타낸다.<br>

Definition : 확률공간 O와 모든 집합이 가측 집합인 가측 공간 E가 주어졌을 때, 메모리 k의 마르코프 연쇄는 다음 성질을 만족시키는 일련의 확률변수 X1, X2, ... : O -> E 이다.

### More Explanation<br>
여러 State를 갖는 Chain 형태의 구조 <br>
State가 존재하고, 각 States를 넘나드는 어떤 확률값이 존재하며, 다음 State는 현재 State값에만 의존한다면(Markov Property)
Markov Chain이라고 한다. <br>

### Markov Property
과거와 현재 상태가 주어졌을 때 미래 상태의 조건부 확률 분포가 과거 상태와는 독립적으로 현재 상태에 의해서만 결정된다는 것을 뜻한다.
```
어떤 정수 n과 임의의 state i0 ~ in에 대해
P(Xn​=in​∣Xn−1​=in−1​) = P(Xn​=in​∣X0​=i0​,X1​=i1​,…,Xn−1​=in−1​)
```

## Markov Chain Monte Carlo (MCMC)
몬테카를로 근사를 하는 마르코프 체인.<br>
임의의 표본에서 시작해서 i번째 표본을 참고해서 i+1번째 표본을 추출함<br>
[좀 더 직관적인 설명이 필요하다면](http://www.secmem.org/blog/2019/01/11/mcmc/), 
[코드기반 설명](https://twiecki.io/blog/2015/11/10/mcmc-sampling/#Setting-up-the-problem), [조금 더 쉬운 코드기반 설명](https://towardsdatascience.com/from-scratch-bayesian-inference-markov-chain-monte-carlo-and-metropolis-hastings-in-python-ef21a29e25a)

### Sampling?
임의의 확률 분포 P(x)에서 표본을 추출하는 작업 <br>
표본은 확률분포를 반영하므로 전체 표본에서 각 상태의 출현 횟수는 자신의 확률분포 값에 비례한다.

즉, MCMC (Markov Chain Monte Carlo)는 Markov Chain의 특수한 경우로 Unique Stationary Distribution과 Limiting Distribution을 가질 경우 MCMC라고 따로 부른다.<br>

### Stationary Distribution?
aP = a 형태를 만족하는 a를 Stationary Distribution이라고 한다. (a는 임의의 vector, P는 Transition Matrix)<br>
어떤 Transition 확률이 존재할 떄, vector a가 얼마 만큼의 Transition이 이뤄지더라도 a가 된다 = 특정 State가 발생할 확률이 항상 같다. 따라서 다음과 같은 세 가지 조건을 만족한다.
```
1. Node 간의 주기성이 없다. (Aperiodic)
2. State 에서 다음 State로 Transition 할 확률이 모두 0보다 크다. (Irreducible)
3. State i를 지나친 cycle에 대해 유한시간 내 다시 State i를 방문할 수 있어야 한다. (Positive Recurrent)
(Irreducible 하더라도 확률이 상대적으로 매우 작은 경우에는 State에 갇혀 재방문이 어려울 수 있다)
```
Stationary Distribution을 만족하는 Markov Chain을 찾는 쉬운 방법(=MCMC를 찾는 방법)으로 detailed balance를 만족시키는지를 확인하는 방법이 있다.

Detailed balance : p(x)T(y|x)=p(y)T(x|y) / T는 Transition distribution [좀 더 자세한 설명이 필요하다면](https://m.blog.naver.com/sw4r/221275103804)

즉, 여기서 y를 x의 다음 상태라고 한다면 MCMC가 되는 것이다.

### Metropolis-Hastings Algorithm (MH Algorithm)
MCMC를 만족하는 Sampling을 진행하는 기본적인 알고리즘. / 다른 방법으로는 Gibbs Sampling이 존재한다.
```
1. Target distribution이 p(x)이고 MH는 p(x)에 비례하는 어떤 함수 f(x)를 계산할 수 있다면
   p(x)는 MCMC Sampling할 수 있는 알고리즘이다. (p(x)를 알고 있느냐와는 상관 없다.)
2. f(x)만으로는 p(x)를 구할 수 없기 때문에, q(x^t|x)와 같은 쉽게 샘플링이 가능한 조건부 확률 분포가 더 필요하다.
   (x를 참고해서 x^t를 예측하기 위한 조건부 확률)
3. Transition distribution T(x^t|x)는 구하기 어렵고 샘플링도 불가하므로 q(x^t|x)를 이용해 현재 샘플 Xi에서
   Xt(t = i + 1)를 추출하고 q(x^t|x)와 T(x^t|x)의 차이를 이용해 X^t를 다음 샘플로 할 수 있는지 결정한다.
4. 이를 위해 Acceptance ratio를 정의한다. (T와 q의 차이를 이용)
   A(x^t|x) = T(x^t|x) / q(x^t|x)
5. 앞서 기술한 Detailed Balance 식{p(x)T(y|x)=p(y)T(x|y)}을 이용해 풀어 쓰면
   A(x^t|x) / A(x|x^t) = {T(x^t|x) / q(x^t|x)} / {T(x|x^t) / q(x|x^t)}
   = p(x^t)q(x|x^t) / p(x)q(x^t|x) = f(x^t)q(x|x^t) / f(x)q(x^t|x)
   Metropolis는 A(x^t|x) = min(1, f(x^t)q(x|x^t) / f(x)q(x^t|x)) 로 했다.
```

### Gibbs Sampling
참고한 문서 : https://ratsgo.github.io/statistics/2017/05/31/gibbs/