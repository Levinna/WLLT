{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # Use torch.nn as nn\n",
    "import torch.nn.functional as F # Use torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # Network for Simple Image Processing\n",
    "    # Spec\n",
    "    # INPUT : 32 * 32 pixels 1 image\n",
    "    # Layer 1 : 3 * 3 square kernel -> 28 * 28 pixels with 6 output channels (Convolution) / 14 * 14 pixels with 6 output channels(Subsampling)\n",
    "    # Layer 2 : 3 * 3 square kernel -> 10 * 10 pixels with 16 output channels (Convolution) / 5 * 5 pixels with 16 output channels(Subsampling)\n",
    "    # Layer 3 : 120 nodes\n",
    "    # Layer 4 : 84 nodes\n",
    "    # Layer 5(OUTPUT) : 10 nodes (labels for 1 to 10)\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 2 Convolution Layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3) # 1 input image channel, 6 output channels, 3 x 3 square convolution\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # 6 input channels, 16 output channels, 3 * 3 square convolution\n",
    "        # 2 Fulley Connected Layers\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 1 Output Layer\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Conv -> ReLu -> Pooling(Max-Pooling)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # 2 -> equals to (2, 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no Activation\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# implementation of NN with numpy\n",
    "# 목표 : random한 input이 random한 output을 학습하는 과정\n",
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension; H is hidden Dimension; D_out is output dimension\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create Random input and output data\n",
    "x = np.random.randn(N, D_in) # randn : rand var with (avg0, std1) as [M, N] array\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward Prop\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop\n",
    "    grad_y_pred = 2.0 * (y_pred - y) # 2*(o-d) : Euclidean Distance의 도함수\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # gradient descent\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # output layer -> hidden layer gradient 전파\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0 # ReLu를 사용하고 있으므로 h < 0인 값은 그 gradient도 0으로 초기화 (전파하지 않음)\n",
    "    grad_w1 = x.T.dot(grad_h) # hidden layer -> input gradient 전파 / 보통 input은 layer로 치지 않는다.\n",
    "    \n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1) # numpy의 dot product와 같은 효과\n",
    "    h_relu = h.clamp(min=0) # (x,0~x) 로 fitting\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99: # 이전과 달리 100번째마다 print\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN based on numpy by learning binary addition operation\n",
    "# Import Libraries\n",
    "import copy, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def sigmoid(x):\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the maximum dimension (8 bit)\n",
    "max_binary_dim = 8\n",
    "largest_number = pow(2, max_binary_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary lookup table\n",
    "# np.unpackbits e.g.\n",
    "print(np.unpackbits(np.array([8], dtype = np.uint8)))\n",
    "print(\"====================\")\n",
    "# 1~256 Integer to nested list\n",
    "# e.g\n",
    "# binary_gonna_be = np.array([range(largest_number)], dtype=np.uint8).T\n",
    "# print(binary_gonna_be)\n",
    "binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis = 1)\n",
    "#print(binary.shape, binary)\n",
    "print(\"====================\")\n",
    "int2binary = {}\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "print(\"lookup table test\")\n",
    "#print(binary[3], int2binary[244])\n",
    "#print(int2binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 #learning rate\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim))-1 # [-1,1) / (b-a) * random_sample() + a\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim))-1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim))-1\n",
    "\n",
    "print(synapse_0.shape, synapse_1.shape, synapse_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[0.05461005]\n",
      "Pred:[1 0 0 0 1 1 0 1]\n",
      "True:[1 0 0 0 1 1 0 1]\n",
      "True\n",
      "32 + 109 = 141\n",
      "------------\n",
      "Error:[0.0339727]\n",
      "Pred:[0 0 1 1 1 1 1 0]\n",
      "True:[0 0 1 1 1 1 1 0]\n",
      "True\n",
      "33 + 29 = 62\n",
      "------------\n",
      "Error:[0.05299518]\n",
      "Pred:[0 0 0 1 1 1 1 1]\n",
      "True:[0 0 0 1 1 1 1 1]\n",
      "True\n",
      "30 + 1 = 31\n",
      "------------\n",
      "Error:[0.07249891]\n",
      "Pred:[1 0 0 1 1 0 1 0]\n",
      "True:[1 0 0 1 1 0 1 0]\n",
      "True\n",
      "55 + 99 = 154\n",
      "------------\n",
      "Error:[0.06553363]\n",
      "Pred:[0 1 0 0 1 0 1 0]\n",
      "True:[0 1 0 0 1 0 1 0]\n",
      "True\n",
      "16 + 58 = 74\n",
      "------------\n",
      "Error:[0.07827149]\n",
      "Pred:[1 0 1 0 1 0 0 0]\n",
      "True:[1 0 1 0 1 0 0 0]\n",
      "True\n",
      "73 + 95 = 168\n",
      "------------\n",
      "Error:[0.08466326]\n",
      "Pred:[0 1 1 0 1 0 0 0]\n",
      "True:[0 1 1 0 1 0 0 0]\n",
      "True\n",
      "58 + 46 = 104\n",
      "------------\n",
      "Error:[0.07566538]\n",
      "Pred:[1 1 1 0 1 0 1 0]\n",
      "True:[1 1 1 0 1 0 1 0]\n",
      "True\n",
      "127 + 107 = 234\n",
      "------------\n",
      "Error:[0.10545346]\n",
      "Pred:[1 0 0 1 1 0 0 0]\n",
      "True:[1 0 0 1 1 0 0 0]\n",
      "True\n",
      "122 + 30 = 152\n",
      "------------\n",
      "Error:[0.06838954]\n",
      "Pred:[0 0 1 0 0 0 0 1]\n",
      "True:[0 0 1 0 0 0 0 1]\n",
      "True\n",
      "15 + 18 = 33\n",
      "------------\n",
      "Error:[0.07745865]\n",
      "Pred:[1 1 1 1 0 1 0 0]\n",
      "True:[1 1 1 1 0 1 0 0]\n",
      "True\n",
      "121 + 123 = 244\n",
      "------------\n",
      "Error:[0.04595762]\n",
      "Pred:[0 1 1 0 1 0 1 0]\n",
      "True:[0 1 1 0 1 0 1 0]\n",
      "True\n",
      "25 + 81 = 106\n",
      "------------\n",
      "Error:[0.07248803]\n",
      "Pred:[1 0 0 0 1 1 0 1]\n",
      "True:[1 0 0 0 1 1 0 1]\n",
      "True\n",
      "103 + 38 = 141\n",
      "------------\n",
      "Error:[0.07100789]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 0 1 0 0 0 1 0]\n",
      "True\n",
      "101 + 61 = 162\n",
      "------------\n",
      "Error:[0.05269783]\n",
      "Pred:[1 1 0 0 0 1 1 0]\n",
      "True:[1 1 0 0 0 1 1 0]\n",
      "True\n",
      "96 + 102 = 198\n",
      "------------\n",
      "Error:[0.07709745]\n",
      "Pred:[1 0 1 1 1 0 0 0]\n",
      "True:[1 0 1 1 1 0 0 0]\n",
      "True\n",
      "91 + 93 = 184\n",
      "------------\n",
      "Error:[0.06981208]\n",
      "Pred:[1 0 0 0 0 1 1 1]\n",
      "True:[1 0 0 0 0 1 1 1]\n",
      "True\n",
      "20 + 115 = 135\n",
      "------------\n",
      "Error:[0.07884457]\n",
      "Pred:[0 1 1 0 1 1 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "True\n",
      "14 + 94 = 108\n",
      "------------\n",
      "Error:[0.04368454]\n",
      "Pred:[0 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "True\n",
      "96 + 30 = 126\n",
      "------------\n",
      "Error:[0.09908387]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "True\n",
      "109 + 27 = 136\n",
      "------------\n",
      "Error:[0.09095689]\n",
      "Pred:[1 1 0 1 0 1 0 0]\n",
      "True:[1 1 0 1 0 1 0 0]\n",
      "True\n",
      "94 + 118 = 212\n",
      "------------\n",
      "Error:[0.03011814]\n",
      "Pred:[0 0 0 1 1 0 0 1]\n",
      "True:[0 0 0 1 1 0 0 1]\n",
      "True\n",
      "16 + 9 = 25\n",
      "------------\n",
      "Error:[0.07607553]\n",
      "Pred:[1 1 0 1 1 0 0 1]\n",
      "True:[1 1 0 1 1 0 0 1]\n",
      "True\n",
      "92 + 125 = 217\n",
      "------------\n",
      "Error:[0.08115457]\n",
      "Pred:[1 0 1 1 1 0 1 1]\n",
      "True:[1 0 1 1 1 0 1 1]\n",
      "True\n",
      "68 + 119 = 187\n",
      "------------\n",
      "Error:[0.06087614]\n",
      "Pred:[0 1 1 0 1 0 1 1]\n",
      "True:[0 1 1 0 1 0 1 1]\n",
      "True\n",
      "101 + 6 = 107\n",
      "------------\n",
      "Error:[0.05154019]\n",
      "Pred:[1 0 1 0 1 1 1 0]\n",
      "True:[1 0 1 0 1 1 1 0]\n",
      "True\n",
      "109 + 65 = 174\n",
      "------------\n",
      "Error:[0.02932747]\n",
      "Pred:[0 0 1 0 0 1 0 1]\n",
      "True:[0 0 1 0 0 1 0 1]\n",
      "True\n",
      "5 + 32 = 37\n",
      "------------\n",
      "Error:[0.07351145]\n",
      "Pred:[1 1 0 0 1 1 0 0]\n",
      "True:[1 1 0 0 1 1 0 0]\n",
      "True\n",
      "123 + 81 = 204\n",
      "------------\n",
      "Error:[0.06500073]\n",
      "Pred:[0 0 1 0 0 1 0 0]\n",
      "True:[0 0 1 0 0 1 0 0]\n",
      "True\n",
      "27 + 9 = 36\n",
      "------------\n",
      "Error:[0.07763976]\n",
      "Pred:[1 0 0 0 1 1 0 0]\n",
      "True:[1 0 0 0 1 1 0 0]\n",
      "True\n",
      "55 + 85 = 140\n",
      "------------\n",
      "Error:[0.07431783]\n",
      "Pred:[1 0 0 0 0 1 1 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "True\n",
      "29 + 105 = 134\n",
      "------------\n",
      "Error:[0.04814233]\n",
      "Pred:[1 0 0 1 0 0 1 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "True\n",
      "80 + 67 = 147\n",
      "------------\n",
      "Error:[0.07887294]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[0 1 0 0 1 0 0 0]\n",
      "True\n",
      "38 + 34 = 72\n",
      "------------\n",
      "Error:[0.05299518]\n",
      "Pred:[0 0 1 1 1 0 0 1]\n",
      "True:[0 0 1 1 1 0 0 1]\n",
      "True\n",
      "11 + 46 = 57\n",
      "------------\n",
      "Error:[0.07222684]\n",
      "Pred:[1 0 0 0 1 1 0 0]\n",
      "True:[1 0 0 0 1 1 0 0]\n",
      "True\n",
      "42 + 98 = 140\n",
      "------------\n",
      "Error:[0.06758392]\n",
      "Pred:[0 1 0 1 0 1 0 0]\n",
      "True:[0 1 0 1 0 1 0 0]\n",
      "True\n",
      "47 + 37 = 84\n",
      "------------\n",
      "Error:[0.0844411]\n",
      "Pred:[1 0 0 1 0 0 1 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "True\n",
      "21 + 126 = 147\n",
      "------------\n",
      "Error:[0.06839109]\n",
      "Pred:[0 1 0 0 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 1 1]\n",
      "True\n",
      "45 + 34 = 79\n",
      "------------\n",
      "Error:[0.05800266]\n",
      "Pred:[0 1 1 0 1 0 0 1]\n",
      "True:[0 1 1 0 1 0 0 1]\n",
      "True\n",
      "3 + 102 = 105\n",
      "------------\n",
      "Error:[0.05484804]\n",
      "Pred:[0 1 0 0 0 1 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "True\n",
      "17 + 52 = 69\n",
      "------------\n",
      "Error:[0.05055438]\n",
      "Pred:[0 1 1 1 0 1 0 1]\n",
      "True:[0 1 1 1 0 1 0 1]\n",
      "True\n",
      "18 + 99 = 117\n",
      "------------\n",
      "Error:[0.07794369]\n",
      "Pred:[1 1 0 1 1 0 0 1]\n",
      "True:[1 1 0 1 1 0 0 1]\n",
      "True\n",
      "126 + 91 = 217\n",
      "------------\n",
      "Error:[0.03987987]\n",
      "Pred:[0 0 1 0 0 1 1 1]\n",
      "True:[0 0 1 0 0 1 1 1]\n",
      "True\n",
      "32 + 7 = 39\n",
      "------------\n",
      "Error:[0.03768956]\n",
      "Pred:[0 0 0 1 0 1 1 0]\n",
      "True:[0 0 0 1 0 1 1 0]\n",
      "True\n",
      "20 + 2 = 22\n",
      "------------\n",
      "Error:[0.08213737]\n",
      "Pred:[1 1 1 1 1 0 0 0]\n",
      "True:[1 1 1 1 1 0 0 0]\n",
      "True\n",
      "127 + 121 = 248\n",
      "------------\n",
      "Error:[0.05383751]\n",
      "Pred:[1 0 0 0 0 1 1 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "True\n",
      "101 + 33 = 134\n",
      "------------\n",
      "Error:[0.08806155]\n",
      "Pred:[1 0 0 1 0 1 1 0]\n",
      "True:[1 0 0 1 0 1 1 0]\n",
      "True\n",
      "28 + 122 = 150\n",
      "------------\n",
      "Error:[0.02539299]\n",
      "Pred:[0 1 0 1 1 0 1 0]\n",
      "True:[0 1 0 1 1 0 1 0]\n",
      "True\n",
      "17 + 73 = 90\n",
      "------------\n",
      "Error:[0.03565833]\n",
      "Pred:[0 1 1 0 1 0 1 0]\n",
      "True:[0 1 1 0 1 0 1 0]\n",
      "True\n",
      "66 + 40 = 106\n",
      "------------\n",
      "Error:[0.05228988]\n",
      "Pred:[0 0 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "True\n",
      "7 + 56 = 63\n",
      "------------\n",
      "Error:[0.05651811]\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "True\n",
      "52 + 51 = 103\n",
      "------------\n",
      "Error:[0.08443154]\n",
      "Pred:[0 1 1 0 0 0 1 0]\n",
      "True:[0 1 1 0 0 0 1 0]\n",
      "True\n",
      "70 + 28 = 98\n",
      "------------\n",
      "Error:[0.06703749]\n",
      "Pred:[0 1 0 0 1 1 1 0]\n",
      "True:[0 1 0 0 1 1 1 0]\n",
      "True\n",
      "30 + 48 = 78\n",
      "------------\n",
      "Error:[0.07713097]\n",
      "Pred:[1 0 0 0 1 0 0 1]\n",
      "True:[1 0 0 0 1 0 0 1]\n",
      "True\n",
      "39 + 98 = 137\n",
      "------------\n",
      "Error:[0.05584028]\n",
      "Pred:[1 0 0 1 1 1 1 0]\n",
      "True:[1 0 0 1 1 1 1 0]\n",
      "True\n",
      "92 + 66 = 158\n",
      "------------\n",
      "Error:[0.06842841]\n",
      "Pred:[0 1 0 0 0 0 1 0]\n",
      "True:[0 1 0 0 0 0 1 0]\n",
      "True\n",
      "13 + 53 = 66\n",
      "------------\n",
      "Error:[0.0407431]\n",
      "Pred:[0 1 1 0 1 1 1 1]\n",
      "True:[0 1 1 0 1 1 1 1]\n",
      "True\n",
      "68 + 43 = 111\n",
      "------------\n",
      "Error:[0.05474852]\n",
      "Pred:[0 1 1 1 0 1 0 0]\n",
      "True:[0 1 1 1 0 1 0 0]\n",
      "True\n",
      "13 + 103 = 116\n",
      "------------\n",
      "Error:[0.09300662]\n",
      "Pred:[1 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "True\n",
      "79 + 49 = 128\n",
      "------------\n",
      "Error:[0.0795711]\n",
      "Pred:[1 0 1 0 0 0 0 1]\n",
      "True:[1 0 1 0 0 0 0 1]\n",
      "True\n",
      "79 + 82 = 161\n",
      "------------\n",
      "Error:[0.0712886]\n",
      "Pred:[1 0 1 0 1 0 0 1]\n",
      "True:[1 0 1 0 1 0 0 1]\n",
      "True\n",
      "67 + 102 = 169\n",
      "------------\n",
      "Error:[0.07201397]\n",
      "Pred:[0 1 1 0 1 1 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "True\n",
      "82 + 26 = 108\n",
      "------------\n",
      "Error:[0.07527553]\n",
      "Pred:[0 1 0 0 0 0 1 1]\n",
      "True:[0 1 0 0 0 0 1 1]\n",
      "True\n",
      "23 + 44 = 67\n",
      "------------\n",
      "Error:[0.04064564]\n",
      "Pred:[0 1 1 1 1 0 1 0]\n",
      "True:[0 1 1 1 1 0 1 0]\n",
      "True\n",
      "106 + 16 = 122\n",
      "------------\n",
      "Error:[0.02579001]\n",
      "Pred:[0 0 1 1 1 0 1 0]\n",
      "True:[0 0 1 1 1 0 1 0]\n",
      "True\n",
      "33 + 25 = 58\n",
      "------------\n",
      "Error:[0.08922259]\n",
      "Pred:[1 0 1 1 0 0 1 0]\n",
      "True:[1 0 1 1 0 0 1 0]\n",
      "True\n",
      "63 + 115 = 178\n",
      "------------\n",
      "Error:[0.07898967]\n",
      "Pred:[1 1 1 0 1 1 0 0]\n",
      "True:[1 1 1 0 1 1 0 0]\n",
      "True\n",
      "122 + 114 = 236\n",
      "------------\n",
      "Error:[0.06761549]\n",
      "Pred:[0 1 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 0 0 1]\n",
      "True\n",
      "5 + 60 = 65\n",
      "------------\n",
      "Error:[0.05102688]\n",
      "Pred:[1 0 1 0 1 1 0 0]\n",
      "True:[1 0 1 0 1 1 0 0]\n",
      "True\n",
      "76 + 96 = 172\n",
      "------------\n",
      "Error:[0.08984394]\n",
      "Pred:[1 0 1 0 1 1 0 1]\n",
      "True:[1 0 1 0 1 1 0 1]\n",
      "True\n",
      "91 + 82 = 173\n",
      "------------\n",
      "Error:[0.06051798]\n",
      "Pred:[0 1 1 1 0 1 1 0]\n",
      "True:[0 1 1 1 0 1 1 0]\n",
      "True\n",
      "104 + 14 = 118\n",
      "------------\n",
      "Error:[0.0753647]\n",
      "Pred:[1 0 1 1 0 0 1 0]\n",
      "True:[1 0 1 1 0 0 1 0]\n",
      "True\n",
      "99 + 79 = 178\n",
      "------------\n",
      "Error:[0.04853081]\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "True\n",
      "12 + 108 = 120\n",
      "------------\n",
      "Error:[0.04756953]\n",
      "Pred:[1 0 1 1 1 1 0 1]\n",
      "True:[1 0 1 1 1 1 0 1]\n",
      "True\n",
      "68 + 121 = 189\n",
      "------------\n",
      "Error:[0.08180813]\n",
      "Pred:[1 1 0 1 0 1 0 1]\n",
      "True:[1 1 0 1 0 1 0 1]\n",
      "True\n",
      "107 + 106 = 213\n",
      "------------\n",
      "Error:[0.04111672]\n",
      "Pred:[0 1 1 0 1 1 1 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "True\n",
      "76 + 34 = 110\n",
      "------------\n",
      "Error:[0.06801002]\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "True\n",
      "34 + 86 = 120\n",
      "------------\n",
      "Error:[0.04325355]\n",
      "Pred:[0 0 1 0 0 1 1 0]\n",
      "True:[0 0 1 0 0 1 1 0]\n",
      "True\n",
      "19 + 19 = 38\n",
      "------------\n",
      "Error:[0.06848438]\n",
      "Pred:[0 0 1 1 0 1 0 0]\n",
      "True:[0 0 1 1 0 1 0 0]\n",
      "True\n",
      "30 + 22 = 52\n",
      "------------\n",
      "Error:[0.03870872]\n",
      "Pred:[0 1 1 1 1 0 1 0]\n",
      "True:[0 1 1 1 1 0 1 0]\n",
      "True\n",
      "16 + 106 = 122\n",
      "------------\n",
      "Error:[0.05113051]\n",
      "Pred:[0 0 1 1 0 1 1 0]\n",
      "True:[0 0 1 1 0 1 1 0]\n",
      "True\n",
      "14 + 40 = 54\n",
      "------------\n",
      "Error:[0.05189534]\n",
      "Pred:[0 1 0 1 0 0 1 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "True\n",
      "77 + 5 = 82\n",
      "------------\n",
      "Error:[0.04364159]\n",
      "Pred:[0 1 0 0 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 1 1]\n",
      "True\n",
      "9 + 70 = 79\n",
      "------------\n",
      "Error:[0.07774951]\n",
      "Pred:[1 1 1 0 0 0 0 1]\n",
      "True:[1 1 1 0 0 0 0 1]\n",
      "True\n",
      "119 + 106 = 225\n",
      "------------\n",
      "Error:[0.06478481]\n",
      "Pred:[1 0 1 0 1 0 1 0]\n",
      "True:[1 0 1 0 1 0 1 0]\n",
      "True\n",
      "68 + 102 = 170\n",
      "------------\n",
      "Error:[0.06286864]\n",
      "Pred:[1 1 0 0 0 1 0 1]\n",
      "True:[1 1 0 0 0 1 0 1]\n",
      "True\n",
      "89 + 108 = 197\n",
      "------------\n",
      "Error:[0.05799193]\n",
      "Pred:[1 0 0 0 1 0 0 1]\n",
      "True:[1 0 0 0 1 0 0 1]\n",
      "True\n",
      "49 + 88 = 137\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[0.07524478]\n",
      "Pred:[1 0 1 0 1 1 1 1]\n",
      "True:[1 0 1 0 1 1 1 1]\n",
      "True\n",
      "54 + 121 = 175\n",
      "------------\n",
      "Error:[0.06113549]\n",
      "Pred:[1 0 0 1 0 0 1 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "True\n",
      "98 + 49 = 147\n",
      "------------\n",
      "Error:[0.06891355]\n",
      "Pred:[0 1 1 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 0 0]\n",
      "True\n",
      "1 + 95 = 96\n",
      "------------\n",
      "Error:[0.05577713]\n",
      "Pred:[0 1 0 1 0 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "True\n",
      "50 + 37 = 87\n",
      "------------\n",
      "Error:[0.06524641]\n",
      "Pred:[0 0 1 1 1 1 0 1]\n",
      "True:[0 0 1 1 1 1 0 1]\n",
      "True\n",
      "19 + 42 = 61\n",
      "------------\n",
      "Error:[0.06793776]\n",
      "Pred:[1 1 1 0 0 1 1 1]\n",
      "True:[1 1 1 0 0 1 1 1]\n",
      "True\n",
      "118 + 113 = 231\n",
      "------------\n",
      "Error:[0.05317041]\n",
      "Pred:[1 0 1 1 1 1 1 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "True\n",
      "76 + 114 = 190\n",
      "------------\n",
      "Error:[0.07187298]\n",
      "Pred:[0 1 1 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 0 0]\n",
      "True\n",
      "84 + 12 = 96\n",
      "------------\n",
      "Error:[0.05195108]\n",
      "Pred:[0 1 0 1 1 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "True\n",
      "65 + 23 = 88\n",
      "------------\n",
      "Error:[0.07583068]\n",
      "Pred:[1 1 0 1 0 0 1 1]\n",
      "True:[1 1 0 1 0 0 1 1]\n",
      "True\n",
      "87 + 124 = 211\n",
      "------------\n",
      "Error:[0.04229107]\n",
      "Pred:[1 0 1 1 1 0 1 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "True\n",
      "105 + 81 = 186\n",
      "------------\n",
      "Error:[0.06285183]\n",
      "Pred:[0 0 1 0 0 0 1 1]\n",
      "True:[0 0 1 0 0 0 1 1]\n",
      "True\n",
      "10 + 25 = 35\n",
      "------------\n",
      "Error:[0.04283321]\n",
      "Pred:[0 1 0 0 1 0 1 0]\n",
      "True:[0 1 0 0 1 0 1 0]\n",
      "True\n",
      "71 + 3 = 74\n",
      "------------\n",
      "Error:[0.06722905]\n",
      "Pred:[0 1 1 1 0 1 1 1]\n",
      "True:[0 1 1 1 0 1 1 1]\n",
      "True\n",
      "14 + 105 = 119\n",
      "------------\n",
      "Error:[0.03605245]\n",
      "Pred:[0 1 1 1 0 0 1 1]\n",
      "True:[0 1 1 1 0 0 1 1]\n",
      "True\n",
      "17 + 98 = 115\n",
      "------------\n",
      "Error:[0.05215109]\n",
      "Pred:[1 0 1 0 1 1 0 0]\n",
      "True:[1 0 1 0 1 1 0 0]\n",
      "True\n",
      "108 + 64 = 172\n",
      "------------\n",
      "Error:[0.04076678]\n",
      "Pred:[0 1 1 0 1 1 1 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "True\n",
      "44 + 66 = 110\n",
      "------------\n",
      "Error:[0.05686589]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "True\n",
      "7 + 49 = 56\n",
      "------------\n",
      "Error:[0.09255832]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "True\n",
      "27 + 109 = 136\n",
      "------------\n",
      "Error:[0.05987511]\n",
      "Pred:[0 1 0 0 0 0 1 1]\n",
      "True:[0 1 0 0 0 0 1 1]\n",
      "True\n",
      "33 + 34 = 67\n",
      "------------\n",
      "Error:[0.05598301]\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "True\n",
      "21 + 99 = 120\n",
      "------------\n",
      "Error:[0.07721975]\n",
      "Pred:[1 1 0 1 1 0 0 1]\n",
      "True:[1 1 0 1 1 0 0 1]\n",
      "True\n",
      "103 + 114 = 217\n",
      "------------\n",
      "Error:[0.05367031]\n",
      "Pred:[1 0 1 1 0 1 0 0]\n",
      "True:[1 0 1 1 0 1 0 0]\n",
      "True\n",
      "99 + 81 = 180\n",
      "------------\n",
      "Error:[0.07625986]\n",
      "Pred:[0 1 1 0 0 1 0 0]\n",
      "True:[0 1 1 0 0 1 0 0]\n",
      "True\n",
      "47 + 53 = 100\n",
      "------------\n",
      "Error:[0.07742252]\n",
      "Pred:[0 1 0 0 0 0 1 1]\n",
      "True:[0 1 0 0 0 0 1 1]\n",
      "True\n",
      "62 + 5 = 67\n",
      "------------\n",
      "Error:[0.10408491]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "True\n",
      "106 + 30 = 136\n",
      "------------\n",
      "Error:[0.08630529]\n",
      "Pred:[1 0 1 1 1 0 0 1]\n",
      "True:[1 0 1 1 1 0 0 1]\n",
      "True\n",
      "115 + 70 = 185\n",
      "------------\n",
      "Error:[0.07657456]\n",
      "Pred:[1 0 1 0 1 1 1 1]\n",
      "True:[1 0 1 0 1 1 1 1]\n",
      "True\n",
      "57 + 118 = 175\n",
      "------------\n",
      "Error:[0.07829183]\n",
      "Pred:[1 0 0 0 0 1 1 1]\n",
      "True:[1 0 0 0 0 1 1 1]\n",
      "True\n",
      "61 + 74 = 135\n",
      "------------\n",
      "Error:[0.03807527]\n",
      "Pred:[0 1 0 1 0 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "True\n",
      "20 + 67 = 87\n",
      "------------\n",
      "Error:[0.03286927]\n",
      "Pred:[0 0 1 1 1 0 1 1]\n",
      "True:[0 0 1 1 1 0 1 1]\n",
      "True\n",
      "48 + 11 = 59\n",
      "------------\n",
      "Error:[0.07137838]\n",
      "Pred:[0 0 1 1 0 0 1 0]\n",
      "True:[0 0 1 1 0 0 1 0]\n",
      "True\n",
      "30 + 20 = 50\n",
      "------------\n",
      "Error:[0.05658221]\n",
      "Pred:[0 0 1 0 1 0 1 0]\n",
      "True:[0 0 1 0 1 0 1 0]\n",
      "True\n",
      "27 + 15 = 42\n",
      "------------\n",
      "Error:[0.0704677]\n",
      "Pred:[1 1 1 0 1 0 0 1]\n",
      "True:[1 1 1 0 1 0 0 1]\n",
      "True\n",
      "109 + 124 = 233\n",
      "------------\n",
      "Error:[0.04695528]\n",
      "Pred:[1 1 0 0 1 1 1 0]\n",
      "True:[1 1 0 0 1 1 1 0]\n",
      "True\n",
      "105 + 101 = 206\n",
      "------------\n",
      "Error:[0.06589742]\n",
      "Pred:[1 0 1 0 1 0 1 1]\n",
      "True:[1 0 1 0 1 0 1 1]\n",
      "True\n",
      "82 + 89 = 171\n",
      "------------\n",
      "Error:[0.0477094]\n",
      "Pred:[0 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 1 1 1 1 1]\n",
      "True\n",
      "23 + 72 = 95\n",
      "------------\n",
      "Error:[0.07296131]\n",
      "Pred:[0 1 1 1 0 0 0 1]\n",
      "True:[0 1 1 1 0 0 0 1]\n",
      "True\n",
      "73 + 40 = 113\n",
      "------------\n",
      "Error:[0.0880416]\n",
      "Pred:[1 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "True\n",
      "19 + 109 = 128\n",
      "------------\n",
      "Error:[0.07474104]\n",
      "Pred:[1 1 0 0 0 0 1 1]\n",
      "True:[1 1 0 0 0 0 1 1]\n",
      "True\n",
      "87 + 108 = 195\n",
      "------------\n",
      "Error:[0.07854462]\n",
      "Pred:[1 0 1 0 0 1 0 0]\n",
      "True:[1 0 1 0 0 1 0 0]\n",
      "True\n",
      "101 + 63 = 164\n",
      "------------\n",
      "Error:[0.03574051]\n",
      "Pred:[0 0 0 0 1 1 1 1]\n",
      "True:[0 0 0 0 1 1 1 1]\n",
      "True\n",
      "4 + 11 = 15\n",
      "------------\n",
      "Error:[0.04453484]\n",
      "Pred:[0 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 1 1 1 1 1]\n",
      "True\n",
      "75 + 20 = 95\n",
      "------------\n",
      "Error:[0.07908802]\n",
      "Pred:[1 0 1 1 1 0 0 1]\n",
      "True:[1 0 1 1 1 0 0 1]\n",
      "True\n",
      "67 + 118 = 185\n",
      "------------\n",
      "Error:[0.05182955]\n",
      "Pred:[0 0 0 1 1 0 0 0]\n",
      "True:[0 0 0 1 1 0 0 0]\n",
      "True\n",
      "15 + 9 = 24\n",
      "------------\n",
      "Error:[0.0663466]\n",
      "Pred:[1 1 0 0 1 0 0 0]\n",
      "True:[1 1 0 0 1 0 0 0]\n",
      "True\n",
      "103 + 97 = 200\n",
      "------------\n",
      "Error:[0.08300544]\n",
      "Pred:[1 0 0 0 0 0 1 1]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "True\n",
      "77 + 54 = 131\n",
      "------------\n",
      "Error:[0.04376615]\n",
      "Pred:[0 1 0 1 0 1 1 0]\n",
      "True:[0 1 0 1 0 1 1 0]\n",
      "True\n",
      "9 + 77 = 86\n",
      "------------\n",
      "Error:[0.04455029]\n",
      "Pred:[0 0 1 0 1 1 1 1]\n",
      "True:[0 0 1 0 1 1 1 1]\n",
      "True\n",
      "38 + 9 = 47\n",
      "------------\n",
      "Error:[0.07361419]\n",
      "Pred:[1 0 0 0 0 1 0 0]\n",
      "True:[1 0 0 0 0 1 0 0]\n",
      "True\n",
      "104 + 28 = 132\n",
      "------------\n",
      "Error:[0.06014448]\n",
      "Pred:[1 0 0 1 1 0 1 0]\n",
      "True:[1 0 0 1 1 0 1 0]\n",
      "True\n",
      "85 + 69 = 154\n",
      "------------\n",
      "Error:[0.07112119]\n",
      "Pred:[0 1 1 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 0 0]\n",
      "True\n",
      "51 + 45 = 96\n",
      "------------\n",
      "Error:[0.0388696]\n",
      "Pred:[0 1 0 1 1 1 1 0]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "True\n",
      "23 + 71 = 94\n",
      "------------\n",
      "Error:[0.06120069]\n",
      "Pred:[0 1 1 1 1 0 0 1]\n",
      "True:[0 1 1 1 1 0 0 1]\n",
      "True\n",
      "60 + 61 = 121\n",
      "------------\n",
      "Error:[0.08065275]\n",
      "Pred:[0 1 0 1 1 1 0 0]\n",
      "True:[0 1 0 1 1 1 0 0]\n",
      "True\n",
      "30 + 62 = 92\n",
      "------------\n",
      "Error:[0.0402205]\n",
      "Pred:[0 0 1 0 1 1 1 1]\n",
      "True:[0 0 1 0 1 1 1 1]\n",
      "True\n",
      "37 + 10 = 47\n",
      "------------\n",
      "Error:[0.06480049]\n",
      "Pred:[0 0 1 1 0 0 0 0]\n",
      "True:[0 0 1 1 0 0 0 0]\n",
      "True\n",
      "29 + 19 = 48\n",
      "------------\n",
      "Error:[0.05331562]\n",
      "Pred:[1 1 0 1 0 1 1 1]\n",
      "True:[1 1 0 1 0 1 1 1]\n",
      "True\n",
      "96 + 119 = 215\n",
      "------------\n",
      "Error:[0.05969633]\n",
      "Pred:[1 0 0 0 1 1 1 0]\n",
      "True:[1 0 0 0 1 1 1 0]\n",
      "True\n",
      "113 + 29 = 142\n",
      "------------\n",
      "Error:[0.07715693]\n",
      "Pred:[1 0 0 1 0 1 0 1]\n",
      "True:[1 0 0 1 0 1 0 1]\n",
      "True\n",
      "38 + 111 = 149\n",
      "------------\n",
      "Error:[0.06051148]\n",
      "Pred:[1 1 0 0 1 0 1 1]\n",
      "True:[1 1 0 0 1 0 1 1]\n",
      "True\n",
      "83 + 120 = 203\n",
      "------------\n",
      "Error:[0.0594758]\n",
      "Pred:[1 0 0 0 1 0 0 1]\n",
      "True:[1 0 0 0 1 0 0 1]\n",
      "True\n",
      "24 + 113 = 137\n",
      "------------\n",
      "Error:[0.05731342]\n",
      "Pred:[0 1 1 1 0 0 1 0]\n",
      "True:[0 1 1 1 0 0 1 0]\n",
      "True\n",
      "3 + 111 = 114\n",
      "------------\n",
      "Error:[0.06591733]\n",
      "Pred:[1 0 0 1 0 1 1 0]\n",
      "True:[1 0 0 1 0 1 1 0]\n",
      "True\n",
      "107 + 43 = 150\n",
      "------------\n",
      "Error:[0.07117639]\n",
      "Pred:[1 1 1 0 1 0 0 0]\n",
      "True:[1 1 1 0 1 0 0 0]\n",
      "True\n",
      "116 + 116 = 232\n",
      "------------\n",
      "Error:[0.0812838]\n",
      "Pred:[1 0 1 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "True\n",
      "101 + 59 = 160\n",
      "------------\n",
      "Error:[0.05206314]\n",
      "Pred:[0 1 1 1 0 1 0 1]\n",
      "True:[0 1 1 1 0 1 0 1]\n",
      "True\n",
      "51 + 66 = 117\n",
      "------------\n",
      "Error:[0.0689495]\n",
      "Pred:[1 0 1 0 0 1 1 1]\n",
      "True:[1 0 1 0 0 1 1 1]\n",
      "True\n",
      "118 + 49 = 167\n",
      "------------\n",
      "Error:[0.07145438]\n",
      "Pred:[1 1 0 1 0 1 1 1]\n",
      "True:[1 1 0 1 0 1 1 1]\n",
      "True\n",
      "124 + 91 = 215\n",
      "------------\n",
      "Error:[0.04801239]\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "True\n",
      "34 + 67 = 101\n",
      "------------\n",
      "Error:[0.04701088]\n",
      "Pred:[1 1 0 0 1 1 0 1]\n",
      "True:[1 1 0 0 1 1 0 1]\n",
      "True\n",
      "109 + 96 = 205\n",
      "------------\n",
      "Error:[0.04145554]\n",
      "Pred:[0 1 1 1 1 0 1 1]\n",
      "True:[0 1 1 1 1 0 1 1]\n",
      "True\n",
      "27 + 96 = 123\n",
      "------------\n",
      "Error:[0.05634651]\n",
      "Pred:[0 1 1 1 1 0 1 0]\n",
      "True:[0 1 1 1 1 0 1 0]\n",
      "True\n",
      "91 + 31 = 122\n",
      "------------\n",
      "Error:[0.06118715]\n",
      "Pred:[1 0 1 1 1 0 1 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "True\n",
      "75 + 111 = 186\n",
      "------------\n",
      "Error:[0.0627967]\n",
      "Pred:[0 1 1 0 1 0 0 1]\n",
      "True:[0 1 1 0 1 0 0 1]\n",
      "True\n",
      "15 + 90 = 105\n",
      "------------\n",
      "Error:[0.08297146]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "True\n",
      "28 + 108 = 136\n",
      "------------\n",
      "Error:[0.03260094]\n",
      "Pred:[0 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "True\n",
      "53 + 73 = 126\n",
      "------------\n",
      "Error:[0.07101179]\n",
      "Pred:[1 0 0 0 0 1 0 0]\n",
      "True:[1 0 0 0 0 1 0 0]\n",
      "True\n",
      "50 + 82 = 132\n",
      "------------\n",
      "Error:[0.05042316]\n",
      "Pred:[1 0 1 1 1 0 1 1]\n",
      "True:[1 0 1 1 1 0 1 1]\n",
      "True\n",
      "105 + 82 = 187\n",
      "------------\n",
      "Error:[0.06945543]\n",
      "Pred:[1 0 1 0 1 1 1 1]\n",
      "True:[1 0 1 0 1 1 1 1]\n",
      "True\n",
      "53 + 122 = 175\n",
      "------------\n",
      "Error:[0.07248227]\n",
      "Pred:[0 1 0 1 1 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "True\n",
      "29 + 59 = 88\n",
      "------------\n",
      "Error:[0.03130174]\n",
      "Pred:[0 0 1 1 1 1 1 0]\n",
      "True:[0 0 1 1 1 1 1 0]\n",
      "True\n",
      "43 + 19 = 62\n",
      "------------\n",
      "Error:[0.07533937]\n",
      "Pred:[0 1 0 0 1 0 1 1]\n",
      "True:[0 1 0 0 1 0 1 1]\n",
      "True\n",
      "21 + 54 = 75\n",
      "------------\n",
      "Error:[0.06963873]\n",
      "Pred:[1 0 1 1 1 0 1 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "True\n",
      "108 + 78 = 186\n",
      "------------\n",
      "Error:[0.06738215]\n",
      "Pred:[1 1 0 0 1 1 1 1]\n",
      "True:[1 1 0 0 1 1 1 1]\n",
      "True\n",
      "89 + 118 = 207\n",
      "------------\n",
      "Error:[0.05649501]\n",
      "Pred:[1 0 0 1 1 1 1 1]\n",
      "True:[1 0 0 1 1 1 1 1]\n",
      "True\n",
      "70 + 89 = 159\n",
      "------------\n",
      "Error:[0.06132919]\n",
      "Pred:[0 1 0 1 0 1 0 0]\n",
      "True:[0 1 0 1 0 1 0 0]\n",
      "True\n",
      "40 + 44 = 84\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[0.05810305]\n",
      "Pred:[1 0 0 0 1 1 0 0]\n",
      "True:[1 0 0 0 1 1 0 0]\n",
      "True\n",
      "108 + 32 = 140\n",
      "------------\n",
      "Error:[0.0673381]\n",
      "Pred:[1 0 1 1 0 0 1 1]\n",
      "True:[1 0 1 1 0 0 1 1]\n",
      "True\n",
      "105 + 74 = 179\n",
      "------------\n",
      "Error:[0.08556014]\n",
      "Pred:[1 0 0 1 0 0 1 0]\n",
      "True:[1 0 0 1 0 0 1 0]\n",
      "True\n",
      "59 + 87 = 146\n",
      "------------\n",
      "Error:[0.06049455]\n",
      "Pred:[1 0 0 1 0 1 0 0]\n",
      "True:[1 0 0 1 0 1 0 0]\n",
      "True\n",
      "99 + 49 = 148\n",
      "------------\n",
      "Error:[0.06363026]\n",
      "Pred:[0 0 1 1 0 1 0 0]\n",
      "True:[0 0 1 1 0 1 0 0]\n",
      "True\n",
      "46 + 6 = 52\n",
      "------------\n",
      "Error:[0.07868602]\n",
      "Pred:[1 0 0 1 0 0 0 1]\n",
      "True:[1 0 0 1 0 0 0 1]\n",
      "True\n",
      "42 + 103 = 145\n",
      "------------\n",
      "Error:[0.06580283]\n",
      "Pred:[1 0 0 0 1 0 0 1]\n",
      "True:[1 0 0 0 1 0 0 1]\n",
      "True\n",
      "101 + 36 = 137\n",
      "------------\n",
      "Error:[0.05411791]\n",
      "Pred:[1 0 1 1 1 1 1 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "True\n",
      "119 + 71 = 190\n",
      "------------\n",
      "Error:[0.05906722]\n",
      "Pred:[0 1 1 0 0 0 1 1]\n",
      "True:[0 1 1 0 0 0 1 1]\n",
      "True\n",
      "25 + 74 = 99\n",
      "------------\n",
      "Error:[0.03278891]\n",
      "Pred:[0 1 1 1 0 1 0 1]\n",
      "True:[0 1 1 1 0 1 0 1]\n",
      "True\n",
      "4 + 113 = 117\n",
      "------------\n",
      "Error:[0.0576242]\n",
      "Pred:[0 1 0 0 1 1 0 0]\n",
      "True:[0 1 0 0 1 1 0 0]\n",
      "True\n",
      "52 + 24 = 76\n",
      "------------\n",
      "Error:[0.07973244]\n",
      "Pred:[0 1 0 1 1 0 1 1]\n",
      "True:[0 1 0 1 1 0 1 1]\n",
      "True\n",
      "39 + 52 = 91\n",
      "------------\n",
      "Error:[0.06086717]\n",
      "Pred:[0 1 1 0 1 1 1 1]\n",
      "True:[0 1 1 0 1 1 1 1]\n",
      "True\n",
      "57 + 54 = 111\n",
      "------------\n",
      "Error:[0.06571095]\n",
      "Pred:[0 1 1 0 1 1 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "True\n",
      "89 + 19 = 108\n",
      "------------\n",
      "Error:[0.0609607]\n",
      "Pred:[0 1 1 1 0 0 0 0]\n",
      "True:[0 1 1 1 0 0 0 0]\n",
      "True\n",
      "21 + 91 = 112\n",
      "------------\n",
      "Error:[0.0590883]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "True\n",
      "52 + 29 = 81\n",
      "------------\n",
      "Error:[0.06500208]\n",
      "Pred:[1 0 1 0 0 1 1 1]\n",
      "True:[1 0 1 0 0 1 1 1]\n",
      "True\n",
      "52 + 115 = 167\n",
      "------------\n",
      "Error:[0.05803441]\n",
      "Pred:[0 0 1 0 1 0 0 0]\n",
      "True:[0 0 1 0 1 0 0 0]\n",
      "True\n",
      "9 + 31 = 40\n",
      "------------\n",
      "Error:[0.03946579]\n",
      "Pred:[0 1 0 0 1 0 1 0]\n",
      "True:[0 1 0 0 1 0 1 0]\n",
      "True\n",
      "33 + 41 = 74\n",
      "------------\n",
      "Error:[0.06377569]\n",
      "Pred:[0 1 0 0 0 1 0 0]\n",
      "True:[0 1 0 0 0 1 0 0]\n",
      "True\n",
      "35 + 33 = 68\n",
      "------------\n",
      "Error:[0.04662804]\n",
      "Pred:[0 1 0 0 1 1 0 0]\n",
      "True:[0 1 0 0 1 1 0 0]\n",
      "True\n",
      "73 + 3 = 76\n",
      "------------\n",
      "Error:[0.08695784]\n",
      "Pred:[1 0 0 0 0 0 1 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "True\n",
      "44 + 86 = 130\n",
      "------------\n",
      "Error:[0.05554724]\n",
      "Pred:[0 1 1 0 1 0 1 0]\n",
      "True:[0 1 1 0 1 0 1 0]\n",
      "True\n",
      "55 + 51 = 106\n",
      "------------\n",
      "Error:[0.0601669]\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "True\n",
      "60 + 60 = 120\n",
      "------------\n",
      "Error:[0.05879147]\n",
      "Pred:[1 1 0 1 0 1 1 1]\n",
      "True:[1 1 0 1 0 1 1 1]\n",
      "True\n",
      "114 + 101 = 215\n",
      "------------\n",
      "Error:[0.05592528]\n",
      "Pred:[0 1 0 0 1 1 0 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "True\n",
      "29 + 48 = 77\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "max_iter = 20000 # EPOCH\n",
    "# saving for updates and visualization\n",
    "s0_update = np.zeros(synapse_0.shape) # s0_update = np.zeros_like(synapse_0)\n",
    "s1_update = np.zeros(synapse_1.shape) \n",
    "sh_update = np.zeros(synapse_h.shape) \n",
    "\n",
    "overallError_history = list()\n",
    "accuracy = list()\n",
    "accuracy_history = list()\n",
    "accuracy_count = 0\n",
    "for j in range(max_iter):\n",
    "    # Randomly pick two integers and change it to the binary representation\n",
    "    a_int = np.random.randint(1,largest_number//2) # '//' : quotient by 2\n",
    "    a = int2binary[a_int]\n",
    "    b_int = np.random.randint(1,largest_number//2)\n",
    "    b = int2binary[b_int]\n",
    "    # 실제 정답 계산 및 binary 벡터 저장.\n",
    "    # Calculate the answer and save it as a binary form\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # RNN이 예측한 binary 합의 값 저장할 변수 선언.\n",
    "    # Declare the variable for saving the prediction by RNN\n",
    "    pred = np.zeros_like(c)\n",
    "    \n",
    "    overallError = 0\n",
    "    \n",
    "    output_layer_deltas = list()\n",
    "    hidden_layer_values = list()\n",
    "    hidden_layer_values.append(np.zeros(hidden_dim)) # dim: (1, 16)\n",
    "    \n",
    "    # feed forward !\n",
    "    # 이진수의 가장 낮은 자리수부터 시작해야하므로 reversed로 for문 돌림.\n",
    "    # As you have to calculate from the \"first\" position of the binary number, which stands for the lowest value, loop backward.\n",
    "    # e.g. \n",
    "    # 10(2) + 11(2), for the first iteration: X = [[0,1]] y = [[1]]\n",
    "    for position in reversed(range(max_binary_dim)):\n",
    "        \n",
    "        # RNN에 들어갈 input과 output label 이진수 값 가져오기\n",
    "        # Take the input and output label binary values\n",
    "        X = np.array([[a[position],b[position]]]) # dim: (1, 2), e.g. [[1,0]]\n",
    "        y = np.array([[c[position]]]) # dim: (1, 1), e.g. [[1]]\n",
    "        \n",
    "        # hidden layer 계산하기 h_t = sigmoid(X*W_{hx} + h_{t-1}*W_{hh})\n",
    "        hidden_layer = sigmoid(np.dot(X,synapse_0) + np.dot(hidden_layer_values[-1],synapse_h)) # dim: (1, 16)\n",
    "        # hidden_layer_values[-1] : indexing the last element of hidden_layer_values\n",
    "        \n",
    "        # output_layer 계산하기       \n",
    "        output_layer = sigmoid(np.dot(hidden_layer,synapse_1)) # dim: (1, 1), e.g. [[0.47174173]]\n",
    "        \n",
    "        # error 계산\n",
    "        output_layer_error = y-output_layer # dim: (1, 1) \n",
    "        \n",
    "        # display를 위한 저장 (just for displying error curve)\n",
    "        overallError += np.abs(output_layer_error[0]) # dim: (1, )          \n",
    "        \n",
    "        # 이 후 backpropagation에서 사용될 delta 값 미리 계산하여 저장\n",
    "        # Save it for the later use in backpropagation step        \n",
    "        output_layer_deltas.append((output_layer_error) * sigmoid_output_to_derivative(output_layer))        \n",
    "        \n",
    "        # 현재 자리수에 대한 예측값 저장\n",
    "        # save the prediction by my model on this position\n",
    "        pred[position] = np.round(output_layer[0][0])\n",
    "        \n",
    "        # 현재까지 계산된 hidden layer 저장\n",
    "        # save the hidden layer by appending the values to the list\n",
    "        hidden_layer_values.append(copy.deepcopy(hidden_layer)) \n",
    "    \n",
    "    if (j%100 == 0):\n",
    "        overallError_history.append(overallError[0])\n",
    "    \n",
    "    # 이제 backpropagation !\n",
    "        \n",
    "    # 맨 처음 시작할 때는 현재 시점보다 앞에 있는 hidden layer가 없으므로 delta 값이 0임.  \n",
    "    # As RNN needs to consider the \"future\" hidden layer value to calculate the backpropagation and it does not have the \n",
    "    # value at the first time (at the end of the position where backpropagation starts), we have to initialize it with zeros\n",
    "    future_hidden_layer_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    # backpropagation을 할 때는 이진수의 가장 앞자리수 시점부터 돌아와야 하므로 정상적인 for문\n",
    "    # Now it should go \"backward\" which means an ordinary way in the for loop\n",
    "    for position in range(max_binary_dim):\n",
    "        \n",
    "        # 필요한 값들 다시 불러오고 \n",
    "        # bring what you needs for calculation\n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        hidden_layer = hidden_layer_values[-position-1]\n",
    "        prev_hidden_layer = hidden_layer_values[-position-2]\n",
    "        \n",
    "        # 현재 시점에서 output layer error로부터 돌아오는 gradient 값\n",
    "        # Get the gradients flowing back from the error of my output at this position, or time step\n",
    "        output_layer_delta = output_layer_deltas[-position-1]\n",
    "        \n",
    "        # 현재 시점의 hidden layer에 더해진 gradient를 계산하기 위해서는\n",
    "        # 이전 시점의 hidden layer로부터 돌아오는 error gradient + 현재 시점 output layer로부터 돌아오는 error gradient\n",
    "        # 이 둘의 합에 sigmoid의 derivative 계산해줘야 함\n",
    "        # 이유: h_t = sigmoid(X*W_{hx} + h_{t-1}*W_{hh})를 역전파 하는 것을 생각하면 됨.\n",
    "        # Important part! (Backpropagation)\n",
    "        # Think about the feed forward step you have done before: h_t = sigmoid(X*W_{hx} + h_{t-1}*W_{hh})\n",
    "        hidden_layer_delta = (np.dot(future_hidden_layer_delta,synapse_h.T) + np.dot(output_layer_delta,synapse_1.T)) \\\n",
    "                            * sigmoid_output_to_derivative(hidden_layer)\n",
    "        \n",
    "        # 8자리 모두를 다 계산한 후 gradient의 합을 한 번에 update 해준다. \n",
    "        # 이유: backprop이 아직 다 끝나지 않았는데 중간에 hidden layer의 value가 바뀌면 안됨\n",
    "        # Save the updates until the for loop finishes calculation for every position\n",
    "        # Hidden layer values must be changed ONLY AFTER backpropagation is fully done at every position.\n",
    "        s1_update += np.atleast_2d(hidden_layer).T.dot(output_layer_delta)\n",
    "        sh_update += np.atleast_2d(prev_hidden_layer).T.dot(hidden_layer_delta)\n",
    "        s0_update += X.T.dot(hidden_layer_delta)\n",
    "        \n",
    "        # 다음 position으로 넘어가면 현재 hidden_layer_delta가 future step이 되므로 이를 넣어준다.\n",
    "        # Preparation for the next step. Now the current hidden_layer_delta becomes the future hidden_layer_delta.\n",
    "        future_hidden_layer_delta = hidden_layer_delta\n",
    "\n",
    "    # weight 값들 update (learning rate를 곱하여)\n",
    "    synapse_1 += s1_update*alpha\n",
    "    synapse_0 += s0_update*alpha\n",
    "    synapse_h += sh_update*alpha\n",
    "    \n",
    "    # update value initialization for the new training data (새로운 a,b training 이진수에 대해 계산을 해줘야하므로)\n",
    "    s1_update *= 0\n",
    "    s0_update *= 0    \n",
    "    sh_update *= 0\n",
    "    \n",
    "    # accuracy 계산\n",
    "    check = np.equal(pred,c)\n",
    "    if np.sum(check) == max_binary_dim:\n",
    "        accuracy_count += 1\n",
    "    if (j%100 == 0):\n",
    "        accuracy_history.append(accuracy_count)\n",
    "        accuracy_count = 0\n",
    "    \n",
    "    \n",
    "    if (j % 100 == 0):\n",
    "        print (\"Error:\" + str(overallError))\n",
    "        print (\"Pred:\" + str(pred))  # 예측값\n",
    "        print (\"True:\" + str(c))  # 실제값\n",
    "\n",
    "        final_check = np.equal(pred,c)\n",
    "        print (np.sum(final_check) == max_binary_dim)\n",
    "\n",
    "        out = 0\n",
    "\n",
    "        for index, x in enumerate(reversed(pred)):\n",
    "            out += x * pow(2, index)\n",
    "        print (str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print (\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "range(0, 200)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(overallError_history))\n",
    "x_range = range(max_iter//100)\n",
    "\n",
    "print(x_range)\n",
    "\n",
    "plt.plot(x_range,overallError_history,'r-')\n",
    "plt.ylabel('overallError')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x_range,accuracy_history,'b-')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_py37] *",
   "language": "python",
   "name": "conda-env-pytorch_py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
