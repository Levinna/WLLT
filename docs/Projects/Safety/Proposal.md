# 0. Text-Preprocessing

# 1. Sentence Representation

## 1.1. Supervised Word Piece Model (Tokenizing)

### 1.1.1. Khaiii
(구)개발배경 : https://tech.kakao.com/2018/12/13/khaiii/

깃헙 : https://github.com/kakao/khaiii

내부 위키페이지도 있으니 참고.


### 1.1.2. Mecab


## 1.2. 

## 1.3.

### 1.3.1. BERT
간략한 개요 : https://ebbnflow.tistory.com/151 <br>
BERT : https://brunch.co.kr/@tristanmhhd/12 <br>
추가 : https://keep-steady.tistory.com/19<br>
논문정리 : https://mino-park7.github.io/nlp/2018/12/12/bert-%EB%85%BC%EB%AC%B8%EC%A0%95%EB%A6%AC/?fbclid=IwAR3S-8iLWEVG6FGUVxoYdwQyA-zG0GpOUzVEsFBd0ARFg4eFXqCyGLznu7w <br>

### 1.3.2. GPT
GPT 1, 2, 3 : https://littlefoxdiary.tistory.com/44 <br>
GPT 1 : https://vanche.github.io/NLP_Pretrained_Model_GPT/<br>
https://greeksharifa.github.io/nlp(natural%20language%20processing)%20/%20rnns/2019/08/21/OpenAI-GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training/<br>
GPT 2 : https://jeongukjae.github.io/posts/1-gpt2-revivew/<br>



### 1.3.3. ELECTRA






#### Awesome Korean NLP
https://awesomeopensource.com/project/datanada/Awesome-Korean-NLP

#### Transformer and Attention
https://medium.com/transformer-attention-mechanism%EC%9D%84-%EC%A4%91%EC%8B%AC%EC%9C%BC%EB%A1%9C/transformer-attention-mechanism%EC%9D%84-%EC%A4%91%EC%8B%AC%EC%9C%BC%EB%A1%9C-9995c6739477