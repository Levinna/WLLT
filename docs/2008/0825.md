# 0825
## 오늘 목표
```
1. HGUML Camp - 4.Convolutional Neural Network

|- Dive into Deep Learning Exercises

2. Deep Learning Specialization

3. Natural Language Processing Specialization

4. AI for Medicine Specialization

5. Digital Signal Processing 1

6. Pythonic Code Practice & Numpy 100 Exercise

7. Kaggle Practice

8. Algorithm & Data Structure Practice

9. Mathematics for Machine Learning

A. Paper Research - Inception V1, FCN, U-Net 공부하기 : 8/30(Sun) Meeting
```

# 1. HGUML Camp - 4. CNN
https://wiki.pathmind.com/convolutional-network
(다이어그램으로 보면서 다시 한 번 복습)

https://cs231n.github.io/convolutional-networks/

1:17:40

Layer Construction of ConvNets

1) Input [width x height x 3-three color channels ;R, G, B-]
<br> >> can be changed with 

2) Conv Layer[correlation operation with n x n kernel filters]

3) Relu Layer[give non-linearity : elementwise operation]

4) Pooling Layer (Pool Layer) [Reduce dimension, Reduce positional variation]

5) FC Layer (Fulley-Connected Layer : Artificial Neural Net) [for classification]


### Casestudy of CNN
https://nmhkahn.github.io/Casestudy-CNN

### 1 x 1 convolution layer usage
https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/

### Optimizer // 추후 보완해서 복습
https://ruder.io/optimizing-gradient-descent/
<br>with code(cs231n) : https://cs231n.github.io/neural-networks-3/

1. GD : update weight by full-batch

2. SGD : update weight by each samples

3. mini-batch Gradient Descent : update weight by mini-batch
<br> https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb

4. Momentum : update weight using momentum

5. Nesterov Accelerated Gradient : update weight using momentum in gradient computation process
<br> https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12

6. Adagrad

7. RMSProp

8. Adam = RMSProp + Momentum

9. Nadam = NAG + Adam

10. AdaDelta = Adagrad + RMSProp + Momentum