# 0825
## 오늘 목표
```
1. HGUML Camp - 4.Convolutional Neural Network

|- Dive into Deep Learning Exercises

2. Deep Learning Specialization

3. Natural Language Processing Specialization

4. AI for Medicine Specialization

5. Digital Signal Processing 1

6. Pythonic Code Practice & Numpy 100 Exercise

7. Kaggle Practice

8. Algorithm & Data Structure Practice

9. Mathematics for Machine Learning

A. Paper Research - Inception V1, FCN, U-Net 공부하기 : 8/30(Sun) Meeting
```

# 1. HGUML Camp - 4. CNN
https://wiki.pathmind.com/convolutional-network
(다이어그램으로 보면서 다시 한 번 복습)

https://cs231n.github.io/convolutional-networks/

1:17:40

Layer Construction of ConvNets

1) Input [width x height x 3-three color channels ;R, G, B-]
<br> >> can be changed with 

2) Conv Layer[correlation operation with n x n kernel filters]

3) Relu Layer[give non-linearity : elementwise operation]

4) Pooling Layer (Pool Layer) [Reduce dimension, Reduce positional variation]

5) FC Layer (Fulley-Connected Layer : Artificial Neural Net) [for classification]


### Casestudy of CNN
https://nmhkahn.github.io/Casestudy-CNN

### Gentle Introduction to 1 x 1 convolution layer
https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/

### Optimizer // 추후 보완해서 복습
https://ruder.io/optimizing-gradient-descent/
<br>with code(cs231n) : https://cs231n.github.io/neural-networks-3/

1. GD : update weight by full-batch

2. SGD : update weight by each samples

3. mini-batch Gradient Descent : update weight by mini-batch
<br> https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb

4. Momentum : update weight using momentum

5. Nesterov Accelerated Gradient : update weight using momentum in gradient computation process
<br> https://towardsdatascience.com/learning-parameters-part-2-a190bef2d12

6. Adagrad

7. RMSProp

8. Adam = RMSProp + Momentum

9. Nadam = NAG + Adam

10. AdaDelta = Adagrad + RMSProp + Momentum

#### Q .Bias를 Learning 하는 알고리즘도 존재할까?


#### Weight Initialization
https://reniew.github.io/13/

#### Dataset Shift
https://data-newbie.tistory.com/354

Internal Covariate Shift

#### Normalization이 필요한 이유
https://data-newbie.tistory.com/121

### Beginner's Guide to Eigenvector / PCA
https://wiki.pathmind.com/eigenvector

#### Batch Normalization
Andrew Ng 교수님의 설명 : https://www.youtube.com/watch?v=tNIpEZLv_eg
```
y = (x - b) / c
-> Inverse
x = (y - b) / c
c * x = y - b
c * x + b = y
y = c * x + b
```

How does Batch Normalization Help Optimization? : https://www.youtube.com/watch?v=ZOabsYbmBRM

# 2. NLP Specialization Week 1

## 2.2 Negative and Positive Frequencies

Positive and negative counts => Frequency Dictionary

## 2.3 Feature Extraction with Frequencies

Xm = [1(bias), Sum Pos. Frequencies, Sum Neg. Frequencies]
<br>Feature Extraction

## 2.4 Preprocessing
Stemming

Stop Words / Punctuation