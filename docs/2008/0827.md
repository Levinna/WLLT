# 0825
## TODOs
```
1. HGUML Camp - 5.Emerging Topics

2. Deep Learning Specialization

3. Natural Language Processing Specialization

4. AI for Medicine Specialization

5. Digital Signal Processing 1

6. Pythonic Code Practice & Numpy 100 Exercise

7. Kaggle Practice

8. Algorithm & Data Structure Practice

9. Mathematics for Machine Learning

A. Paper Research - Inception V1, FCN, U-Net 공부하기 : 8/30(Sun) Meeting

B. 책 읽기 : 다시, 수학이 필요한 순간
```

# 0. CNN Revisited

## 0.1 AlexNet
AlexNet의 # of parameters, size of tensors : https://seongkyun.github.io/study/2019/01/25/num_of_parameters/

https://kjhov195.github.io/2020-02-10-CNN_architecture_2/

http://suanmiao.me/2017/Classic-Nets-AlexNet-Structure-and-Analysis/

## 0.2 PCA / Eigenvectors
Beginner's Guide to Eigenvector / PCA : https://wiki.pathmind.com/eigenvector

### 0.2.1 Eigenvector
Eigen : 고유의, Meaning "very own" in Germen (e.g. : mein eigenes Auto : "my very own car")

Linear Transformations : with matrix operations, we can easily change one vector into another vector with multiplying and adding them.

Eigenvector : changes length but not directon
 => already pointing in the same direction that the matrix is pushing all vectors toward.
<br>In formal definition : a vector that responds to a matrix as though that matrix were a scalar coefficient.

### 0.2.2 Mean, Variance
1) Mean : average value of all X's in the set X <br>
mean = sum of X's / num of X <br>
2) Standard Deviation : square root of the average square distance of data points to the mean <br>
s = root(sum of difference betw X's and mean of X / (num of X - 1)) // 1 is just trick for easy computation <br>
3) Variance : the spread, or the amount of difference that data expresses <br>
var(X) = s^2 <br>
4) Covariance : Expectation((X - mean of X)(Y - mean of Y))

## 0.3 Tensor Size / Parameters
1) Tensor size of Convolution layer
Output image = {(Input Width) - Kernel size + 2 * Padding size} / Stride + 1

1-2) Parameter size of Convolution layer
Weight = Kernel size(width)^2 * Num of Channels * Num of Kernels
Bias = Num of Kernels
Parameters = Weight + Bias

2) Tensor size of Pooling layer
Output image = {(Input Width) - Pooling size} / Stride + 1

2-2) Parameter size of Pooling layer
Pooling / Stride / Padding : hyperparameter

3) Parameter size of Fulley Connected layer
Weight = Output image(width)^2 * Num of Kernels * Num of Perceptrons
Bias = Num of Perceptrons
Parameters = Weight + Bias

## 0.4 AlexNet parameter and tensor
Input Image : 227(px) * 227(px) * 3(R,G,B)
Conv-1 layer
 - kernel : 11 * 11
 - stride : 4
 Tensor size : (227 - 11)/4 + 1 = 55 * 55 * 96(kernels) = 290400
 Weights : 11^2 * 3 * 96 = 34848
 Bias : 96
 Parameters : Weights + Bias = 34944
... and so on.

## 0.5 Simple CNN code in numpy


#### 단어장
```
constituent : 구성하는
plural : 복수의, 두 가지 이상의
prioritize : 우선 순위를 매기다, 우선시키다
dimensionality : 차원수 (allowing you to drop some low-variance dimensions)
weave : 짜다, 뜨다, 엮다
bisect : 양분하다
oblong : 직사각형, 타원형
perpendicular : 수직의, 직립한, 직각을 이룬

```