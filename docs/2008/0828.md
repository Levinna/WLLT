# 0828
## TODOs
```
1. HGUML Camp - 5.Emerging Topics

2. Deep Learning Specialization

3. Natural Language Processing Specialization

4. AI for Medicine Specialization

5. Digital Signal Processing 1

6. Pythonic Code Practice & Numpy 100 Exercise

7. Kaggle Practice

8. Algorithm & Data Structure Practice

9. Mathematics for Machine Learning

A. Paper Research - Inception V1, FCN, U-Net 공부하기 : 8/30(Sun) Meeting

B. 책 읽기 : 다시, 수학이 필요한 순간

*** 분야별 학회들 Listup, 시기별 목표 확인

```

# 오늘 목표
```
논문읽기 (At least 3, work 5 if it is possible.)
1. VGGNet - 2 (Fin)
2. Visualizing and Understanding Conv Net (Ing)
3. NIN or GoogleNet (Todo)
4. FCN (Todo)
5. U-Net (Todo)
```

# 0.1 VGGNet Parameter and Tensor
https://stackoverflow.com/questions/28232235/how-to-calculate-the-number-of-parameters-of-convolutional-neural-networks

# 0.2 Visualizing and Understanding Convolutional Networks
### 1. Introduction
Without clear understanding of how an dwhy they work, the development of better models is reduced to trial-and-error.<br>
For the visualization technique, we use a multi-layered Deconvolutionial Network(deconvnet) to project the feature activations back to the input pixel space.
### 1.1 Related work
For higher layers, the invariances are extremely complex so are poorly captured by a simple quadratic approximation.<br>
Our approach provides a non-parametric view of invariance.
### 2. Approach
``` Notations
i = layer
x_i = 2D input image
yhat_i = probability vector
C = classes
N = num of images(training set) {x, y} labeled images

```
Each layer (of network) consists of
```
1) Convolution of the previous layer output(or, in the case of the 1st layer, input image) with a set of learned filters.
2) passing the responses through a ReLU
3) [optionally] Max pooling over local neighborhoods
4) [optionally] A local contrast operation that normalizes the responses across feature maps.
Plus, few of Full-connected networks and final layer with softmax classifier.
```
1) Cross-entropy is used as loss function(Compare betw y and yhat)<br>
Parameter consists of [filters-or can be said kernels-, weight matrices, biases]<br>
2) Used SGD for weight update<br>
### 2.1 Visualization with a Deconvnet
We used deconvnet to map the feature activitiy in intermediate layers back to the input pixel space.<br>
A deconvnet is convnet model that uses the same components in reverse.<br>

https://www.youtube.com/watch?v=ghEmQSxT6tw

# 5. Digital Signal Processing 1

## 5.1. Week 3 (Notebook practice)

Discrete Time Fourier Transform<br>
Discrete Time <-> Continuous Frequency Domain<br>
Discrete Fourier Transform<br>
Discrete Time <-> Discrete Frequency Domain<br>
Continuous Time Fourier Transform<br>
Countinuous Time <-> Countinuous Frequency Domain

In Sampling, we use DFT only. (1 by 1)<br>
N Time domain <-> N Frequency Domain<br>


#### Convolution arithmetic
https://github.com/vdumoulin/conv_arithmetic

#### Implementation of Conv Net in numpy
https://wiseodd.github.io/techblog/2016/07/16/convnet-conv-layer/

#### What are deconvolutional layers?
https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers

#### Computer vision models on PyTorch
https://github.com/osmr/imgclsmob/tree/master/pytorch