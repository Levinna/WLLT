# 0831
## TODOs
```
1. HGUML Camp - 5.Emerging Topics
2. Deep Learning Specialization
3. Natural Language Processing Specialization
4. Digital Signal Processing 1
5. Pythonic Code Practice & Numpy 100 Exercise
6. Kaggle Practice
7. Algorithm & Data Structure Practice
8. Mathematics for Machine Learning
9. AI for Medicine Specialization
A. Paper Research - Inception V1, V2, FCN, U-Net 복습
B. 책 읽기 : 다시, 수학이 필요한 순간
*** 분야별 학회들 Listup, 시기별 목표 확인
```

## 오늘 할 일
```
0. Reinforcement Learning
1. NLP Specialization
2. 딥러닝 응용
3. Paper Reading(TODO A.)
4. Meeting 참석
```

# 0. Reinforcement Learning
최희열교수님
 : 머신러닝 2000 ~ 

## 0.1 Resources(Lecture Material)
[First Half(David Silver's Lecture)](https://www.davidsilver.uk/teaching/)<br>
[Direct Link to Video Lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB)<br>
[Second Half(DL Bootcamp)](https://sites.google.com/view/deep-rl-bootcamp/lectures)

Textbook : <br>
Reinforcement Learning : An Introduction
by R. S. Sutton and A. G. Barto<br>
Big guys in RL<br>
Joel from McGill<br>
Sergey from U.C. Berkely

RL is learning what to do in a situation to maximize a reward.
 : mapping situations to actions.

an agent should find which actions yield the most reward by trying them.
 : actions may affect the reward and the next situation

Humans are kind of RL agents

Artificial Intelligence = RL + DL (silver's def. in 2016)

To implement A.G.I. we need robot in the real situation.
Robot should interact with real part.

RL and DL are orthogonal. (in Approach)

RL can adapt DL, DL can be learned by RL. So they are trying to combine them.

RL is composed of agent and environment.

# 0.2 RL components
```
RL examples : pole balancing problem, AlphaGo, self-driving, etc
1) agent : cart, AlphaGo, autonomous vehicle, etc
2) action : cart movement (left or right), play in Go, steering wheel, etc
3) reward
  3-1) negative reward for falling down of the pole
  3-2) positive (or negative) reward for win (or lose) in Go
  3-3) positive reward for safe driving
  ...
4) state
  4-1) current position and velocity of the pole and the cart
  4-2) current board in Go
  4-3) current sensor input for vehicle
```
If we cannot get real negative reward, we should have some tricks.

# 0.3 Characteristics of RL
```
1) compared to supervised learning
  1-1) the current decision affects the next input. (through environment) // Supervised learning, we consider everything is i.i.d.
  1-2) only reward instead of target label is available.
  1-3) the reward might be delayed. // in SL we immediately get the target label.

2) trial-and-error learning
  2-1) trade-off between exploration and exploitation
  exploration : try other paths
  exploitation : when you think it is the best path, you choose that path everytime (without learning anymore)
  2-2) e.g., find the best path to your office from home

3) credit assignment problem (due to the delayed reward)
which one is good, and which one is bad??
  3-1) in sequential decision making
  3-2) immediate reward vs. long-term reward
  3-3) e.g., how do we know which move (or play) is good one in Go?
```

# 0.4 Outline
```
1) Markov Decision Process (MDP)
2) Dynamic Programming (DP)
  divide-and-conquer the problems
                    Dynamic Programming
--------------------------------
                    Sampling Methods
3) On-Policy Methods
  3-1) Monte Carlo (MC)
  3-2) Temporal Difference (TD)
4) Off-Policy Methods
  4-1) Q-learning : based on the table e.g. n-gram in NLP
  4-2) Deep Q Networks : Yoshua Bengio proposed
                    value-based
--------------------------------
                    policy-based
5) Policy Gradient
  5-1) Stochastic Policy Gradient (e.g., REINFORCE)
  5-2) Actor-Critic Policy Gradient
6) Model-Based RL : model free based
```
