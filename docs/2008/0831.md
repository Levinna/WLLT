# 0831
## TODOs
```
1. HGUML Camp - 5.Emerging Topics
2. Deep Learning Specialization
3. Natural Language Processing Specialization
4. Digital Signal Processing 1
5. Pythonic Code Practice & Numpy 100 Exercise
6. Kaggle Practice
7. Algorithm & Data Structure Practice
8. Mathematics for Machine Learning
9. AI for Medicine Specialization
A. Paper Research - Inception V1, V2, FCN, U-Net 복습
B. 책 읽기 : 다시, 수학이 필요한 순간
*** 분야별 학회들 Listup, 시기별 목표 확인
```

## 오늘 할 일
```
0. Reinforcement Learning
1. NLP Specialization
2. 딥러닝 응용
3. Paper Reading(TODO A.)
4. Meeting 참석
```

# 0. Reinforcement Learning
최희열교수님
 : 머신러닝 2000 ~ 

## 0.1 Resources(Lecture Material)
[First Half(David Silver's Lecture)](https://www.davidsilver.uk/teaching/)<br>
[Direct Link to Video Lecture](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB)<br>
[Second Half(DL Bootcamp)](https://sites.google.com/view/deep-rl-bootcamp/lectures)<br>
[Class TextBook](http://incompleteideas.net/book/the-book.html)<br>
[Other TextBooks](https://tensorflow.blog/tag/richard-s-sutton/)

Big guys in RL<br>
Joel from McGill<br>
Sergey from U.C. Berkely

RL is learning what to do in a situation to maximize a reward.
 : mapping situations to actions.

an agent should find which actions yield the most reward by trying them.
 : actions may affect the reward and the next situation

Humans are kind of RL agents

Artificial Intelligence = RL + DL (silver's def. in 2016)

To implement A.G.I. we need robot in the real situation.
Robot should interact with real part.

RL and DL are orthogonal. (in Approach)

RL can adapt DL, DL can be learned by RL. So they are trying to combine them.

RL is composed of agent and environment.

# 0.2 RL components
```
RL examples : pole balancing problem, AlphaGo, self-driving, etc
1) agent : cart, AlphaGo, autonomous vehicle, etc
2) action : cart movement (left or right), play in Go, steering wheel, etc
3) reward
  3-1) negative reward for falling down of the pole
  3-2) positive (or negative) reward for win (or lose) in Go
  3-3) positive reward for safe driving
  ...
4) state
  4-1) current position and velocity of the pole and the cart
  4-2) current board in Go
  4-3) current sensor input for vehicle
```
If we cannot get real negative reward, we should have some tricks.

# 0.3 Characteristics of RL
```
1) compared to supervised learning
  1-1) the current decision affects the next input. (through environment) // Supervised learning, we consider everything is i.i.d.
  1-2) only reward instead of target label is available.
  1-3) the reward might be delayed. // in SL we immediately get the target label.

2) trial-and-error learning
  2-1) trade-off between exploration and exploitation
  exploration : try other paths
  exploitation : when you think it is the best path, you choose that path everytime (without learning anymore)
  2-2) e.g., find the best path to your office from home

3) credit assignment problem (due to the delayed reward)
which one is good, and which one is bad??
  3-1) in sequential decision making
  3-2) immediate reward vs. long-term reward
  3-3) e.g., how do we know which move (or play) is good one in Go?
```

# 0.4 Outline
```
1) Markov Decision Process (MDP)
2) Dynamic Programming (DP)
  divide-and-conquer the problems
                    Dynamic Programming
--------------------------------
                    Sampling Methods
3) On-Policy Methods
  3-1) Monte Carlo (MC)
  3-2) Temporal Difference (TD)
4) Off-Policy Methods
  4-1) Q-learning : based on the table e.g. n-gram in NLP
  4-2) Deep Q Networks : Yoshua Bengio proposed
                    value-based
--------------------------------
                    policy-based
5) Policy Gradient
  5-1) Stochastic Policy Gradient (e.g., REINFORCE)
  5-2) Actor-Critic Policy Gradient
6) Model-Based RL : model free based
```

# 0.5 RL Course by David Silver : Lecture 1
https://www.youtube.com/watch?v=2pWv7GOvuf0

47:54

# 1. NLP Specialization

## Utils Function for Handle Tweets
<details markdown ="1">
<summary>[Python] 코드 보기 / 숨기기</summary>

```python
def process_tweet(tweet):
    """Process tweet function.
    Input:
        tweet: a string containing a tweet
    Output:
        tweets_clean: a list of words containing the processed tweet

    """
    stemmer = PorterStemmer()
    stopwords_english = stopwords.words('english')
    # remove stock market tickers like $GE
    tweet = re.sub(r'\$\w*', '', tweet)
    # remove old style retweet text "RT"
    tweet = re.sub(r'^RT[\s]+', '', tweet)
    # remove hyperlinks
    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
    # remove hashtags
    # only removing the hash # sign from the word
    tweet = re.sub(r'#', '', tweet)
    # tokenize tweets
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,
                               reduce_len=True)
    tweet_tokens = tokenizer.tokenize(tweet)

    tweets_clean = []
    for word in tweet_tokens:
        if (word not in stopwords_english and  # remove stopwords
                word not in string.punctuation):  # remove punctuation
            # tweets_clean.append(word)
            stem_word = stemmer.stem(word)  # stemming word
            tweets_clean.append(stem_word)

    return tweets_clean

def build_freqs(tweets, ys):
    """Build frequencies.
    Input:
        tweets: a list of tweets
        ys: an m x 1 array with the sentiment label of each tweet
            (either 0 or 1)
    Output:
        freqs: a dictionary mapping each (word, sentiment) pair to its
        frequency
    """
    # Convert np array to list since zip needs an iterable.
    # The squeeze is necessary or the list ends up with one element.
    # Also note that this is just a NOP if ys is already a list.
    yslist = np.squeeze(ys).tolist()

    # Start with an empty dictionary and populate it by looping over all tweets
    # and over all processed words in each tweet.
    freqs = {}
    for y, tweet in zip(yslist, tweets):
        for word in process_tweet(tweet):
            pair = (word, y)
            if pair in freqs:
                freqs[pair] += 1
            else:
                freqs[pair] = 1

    return freqs
```

</details>

```python
# Some error analysis done for you
print('Label Predicted Tweet')
for x,y in zip(test_x,test_y):
    y_hat = predict_tweet(x, freqs, theta)

    if np.abs(y - (y_hat > 0.5)) > 0:
        print("0",y_hat)
        print("TEST",(y_hat > 0.5))
        print("TEST2",y - (y_hat > 0.5))
        print("TEST3",np.abs(y - (y_hat > 0.5)))
        print("TEST4",np.abs(y - (y_hat > 0.5)) > 0)
        print('THE TWEET IS:', x)
        print('THE PROCESSED TWEET IS:', process_tweet(x))
        print('%d\t%0.8f\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))
```